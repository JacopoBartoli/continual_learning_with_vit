# Continual Learning with Vistion Transformers
This repo is the official implementation of our CVPR 2022 workshop paper "Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization".

<div align="center">
<img src="./docs/_static/att_fun.png" width="400px">

## Running the code

Scripts for our experiments can be found in `scripts/`.

## Cite
If you found our implementation to be useful, feel free to use the citation:
```bibtex
@inproceedings{pelosin2022towards,
  title={Towards exemplar-free continual learning in vision transformers: an account of attention, functional and weight regularization},
  author={Pelosin, Francesco and Jha, Saurav and Torsello, Andrea and Raducanu, Bogdan and van de Weijer, Joost},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3820--3829},
  year={2022}
}
```

This repo is based on [FACIL](https://github.com/mmasana/FACIL).

<div align="center">
<img src="./docs/_static/facil_logo.png" width="100px">
